# Home_Sales
Module 22 Challenge  - UWA/edX Data Analytics Bootcamp

Github repository at: [https://github.com/alyssahondrade/Home_Sales.git](https://github.com/alyssahondrade/Home_Sales.git)


## Table of Contents
1. [Introduction](https://github.com/alyssahondrade/Home_Sales#introduction)
    1. [Goal](https://github.com/alyssahondrade/Home_Sales#goal)
    2. [Repository Structure](https://github.com/alyssahondrade/Home_Sales#repository-structure)
    3. [Dataset](https://github.com/alyssahondrade/Home_Sales#dataset)
2. [Approach](https://github.com/alyssahondrade/Home_Sales#approach)


## Introduction

### Goal
The goal is use SparkSQL to determine key metrics about home sales data.

### Repository Structure
The root directory contains
- The source code: `Home_Sales.ipynb`.
- The `home_sales_partitioned` directory.

### Dataset
Data for this dataset was generated by __edX Boot Camps LLC__.


## Approach
1. Read the CSV file to a Spark DataFrame
    - Set: `inferSchema = True`
    - Set: `timestampFormat = "yyyy-MM-dd"`
    
2. Create a temporary view using: `createOrReplaceTempView()`

3. Write the SQL queries for the following questions:
    - What is the average price for a four bedroom house sold in each year roudned to two decimal places?
    - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?
    - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?

4. Determine the runtime for the following query:
    - What is the "view" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000?

5. Cache the temporary table and check if it cached correctly.

6. Run the same query as in Step 4, comparing the cached and uncached runtimes.
    - Runtimes:
        - Uncached = `0.16 seconds`
        - Cached = `0.12 seconds`
    - The runtime for the cached data is slightly better than the uncached, faster by .04 seconds.

7. Partition by the `date_built` field on the formatted parquet home sales data and create a temporary view.

8. Run the same query as in Step 4, with the partitioned parquet DataFrame, comparing the runtimes.
    - Runtime: `0.33 seconds`
    - The partitioned parquet data took almost 3 times longer than the cached version. This is likely due to the dataset being partitioned on the `date_built` field but the query is grouped by `view` instead.

9. Uncache the temporary table and check if it uncached correctly.